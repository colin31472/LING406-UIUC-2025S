{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "426f50f4",
   "metadata": {},
   "source": [
    "# Ling406 Sentiment Analysis Term Project\n",
    "## Youngjun Yu\n",
    "\n",
    "**Baseline System**: Simple DNN\n",
    "\n",
    "**Algorithm Selection**: LSTM / CNN / Transformer\n",
    "\n",
    "**Feature Engineering Experiment**: Random Embedding / GloVe Embedding (frozen) / GloVe Embedding (fine‑tuning) / TF‑IDF weighted Average Embeddings\n",
    "\n",
    "**Dataset**:\n",
    "- movie review (Pang/Lee 2004 polarity v2.0)\n",
    "- Yelp review (over 3.5 ⇒ positive, under ⇒ negative) (for extra-crdit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8108100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, glob, tarfile\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "827d0173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5\n",
    "LR = 1e-3\n",
    "EMBED_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "VOCAB_SIZE = 10000\n",
    "MAX_SEQ_LEN = 200\n",
    "DROPOUT_PROB = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "99f84895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer and Vocabulary\n",
    "\n",
    "# Simple whitespace tokenizer \n",
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "# Build vocabulary from a list of texts\n",
    "def build_vocab(texts, max_size, min_freq=1):\n",
    "    counter = Counter()\n",
    "    for txt in texts:\n",
    "        counter.update(tokenize(txt))\n",
    "    most_common = []\n",
    "    for w, f in counter.items():\n",
    "        if f >= min_freq:\n",
    "            most_common.append(w)\n",
    "    most_common = sorted(most_common, key=lambda w: -counter[w])[:max_size]\n",
    "    itos = [\"<PAD>\",\"<UNK>\"] + most_common\n",
    "    stoi = {w:i for i,w in enumerate(itos)}\n",
    "    return stoi, itos\n",
    "\n",
    "# Convert a text string into a list of token indices based on stoi mapping\n",
    "def encode(text, stoi):\n",
    "    tokens = tokenize(text)\n",
    "    idxs = []\n",
    "    for t in tokens:\n",
    "        idx = stoi.get(t, stoi[\"<UNK>\"])\n",
    "        idxs.append(idx)\n",
    "    idxs = idxs[:MAX_SEQ_LEN]\n",
    "    if len(idxs) < MAX_SEQ_LEN:\n",
    "        idxs += [stoi[\"<PAD>\"]] * (MAX_SEQ_LEN - len(idxs))\n",
    "    return idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a74cc540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie reviews: 2000\n"
     ]
    }
   ],
   "source": [
    "# Data Preporcessing- Load Pang Lee Movie Reviews Dataset\n",
    "\n",
    "# Unzip the data file, read the text file, and return it along with the labels\n",
    "def load_panglee(base_tar: str = \"../Data/review_polarity.tar.gz\",\n",
    "                extract_dir: str = \"../Data/review_polarity\"):\n",
    "    if not os.path.isdir(extract_dir) or not os.listdir(extract_dir):\n",
    "        os.makedirs(extract_dir, exist_ok=True)\n",
    "        with tarfile.open(base_tar, \"r:gz\") as tar:\n",
    "            tar.extractall(path=extract_dir)\n",
    "\n",
    "    txt_dir = None\n",
    "    for root, dirs, files in os.walk(extract_dir):\n",
    "        if \"txt_sentoken\" in dirs:\n",
    "            txt_dir = os.path.join(root, \"txt_sentoken\")\n",
    "            break\n",
    "\n",
    "    texts, labels = [], []\n",
    "    for label in [\"pos\", \"neg\"]:\n",
    "        pattern = os.path.join(txt_dir, label, \"*.txt\")\n",
    "        for fn in glob.glob(pattern):\n",
    "            with open(fn, encoding=\"utf-8\") as f:\n",
    "                texts.append(f.read())\n",
    "                labels.append(1 if label == \"pos\" else 0)\n",
    "\n",
    "    return texts, labels\n",
    "\n",
    "# Split the dataset into training and testing sets \n",
    "movie_texts, movie_labels = load_panglee()\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    movie_texts,\n",
    "    movie_labels,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=movie_labels\n",
    ")\n",
    "print(\"Movie reviews:\", len(movie_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3f448b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yelp reviews: 10391 Positives: 6066 Negatives: 4325\n"
     ]
    }
   ],
   "source": [
    "# Data Preporcessing- Load Yelp Dataset\n",
    "\n",
    "# Read the all_reviews.txt file inside the ExtraCredit folder and return the reviews and labels\n",
    "def load_yelp(path=\"../Data/ExtraCredit/Yelp/all_reviews.txt\"):\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        data = f.read()\n",
    "    matches = re.compile(\n",
    "        r\"\\{\\{\\{\\s*(\\d)star\\s*\\}\\}\\}\\s*\\[\\[\\[\\s*(.*?)\\s*\\]\\]\\]\", \n",
    "        re.DOTALL\n",
    "    ).findall(data)\n",
    "\n",
    "    texts = [text.strip() for (_, text) in matches]\n",
    "    labels = [1 if int(r)>=4 else 0 for (r, _) in matches]\n",
    "\n",
    "    return texts, labels\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "yelp_texts, yelp_labels = load_yelp()\n",
    "y_train_texts, y_test_texts, y_train_labels, y_test_labels = train_test_split(\n",
    "    yelp_texts,\n",
    "    yelp_labels,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=yelp_labels\n",
    ")\n",
    "print(\"Yelp reviews:\", len(yelp_texts), \"Positives:\", sum(yelp_labels), \"Negatives:\", len(yelp_labels)-sum(yelp_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "729c6991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 10002\n"
     ]
    }
   ],
   "source": [
    "# Generate Vocabulary\n",
    "\n",
    "stoi, itos = build_vocab(movie_texts, max_size=VOCAB_SIZE)\n",
    "print(\"Vocab size:\", len(itos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "57968b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Dataset and DataLoader\n",
    "\n",
    "# Dataset class for tokenized text inputs and corresponding labels\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, stoi):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.stoi  = stoi\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(encode(self.texts[idx], self.stoi), dtype=torch.long)\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return x,y\n",
    "\n",
    "train_ds = TextDataset(train_texts, train_labels, stoi)\n",
    "test_ds  = TextDataset(test_texts,  test_labels,  stoi)\n",
    "y_train_ds = TextDataset(y_train_texts, y_train_labels, stoi)\n",
    "y_test_ds  = TextDataset(y_test_texts,  y_test_labels,  stoi)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "y_train_loader = DataLoader(y_train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "y_test_loader  = DataLoader(y_test_ds,  batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1ed167f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Models\n",
    "\n",
    "# DNN model with average pooled embeddings\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.fc1 = nn.Linear(emb_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 2)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        pooled = self.embed(x).mean(dim=1)\n",
    "        out = self.drop(torch.relu(self.fc1(pooled)))\n",
    "        return self.fc2(out)\n",
    "\n",
    "# Bidirectional LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim*2, 2)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        emb, _ = self.lstm(self.embed(x))\n",
    "        # Concatenate last forward and first backward hidden states\n",
    "        out = self.drop(torch.cat([emb[:, -1, :self.lstm.hidden_size],\n",
    "                         emb[:,  0, self.lstm.hidden_size:]], dim=1))\n",
    "        return self.fc(out)\n",
    "\n",
    "# CNN Model with multiple filter sizes\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim, dropout, kernel_sizes=[3,4,5], num_filters=100):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        # Create convolution layers with varying kernel sizes\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(1, num_filters, (k, emb_dim)) for k in kernel_sizes\n",
    "        ])\n",
    "        self.fc = nn.Linear(num_filters*len(kernel_sizes), 2)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        emb = self.embed(x).unsqueeze(1)  # [B, 1, L, E]\n",
    "        outs = [torch.relu(conv(emb)).squeeze(3) for conv in self.convs]\n",
    "        pools = [torch.max(o, dim=2)[0] for o in outs]\n",
    "        cat = self.drop(torch.cat(pools, dim=1))\n",
    "        return self.fc(cat)\n",
    "\n",
    "# Positional encoding used in Transformer\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_dim, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, emb_dim)\n",
    "        pos = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, emb_dim, 2) * -(np.log(10000.0)/emb_dim))\n",
    "        pe[:, 0::2] = torch.sin(pos*div)\n",
    "        pe[:, 1::2] = torch.cos(pos*div)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
    "\n",
    "# Transformer Model for text classification\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim, dropout, nhead=4, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.pe = PositionalEncoding(emb_dim)\n",
    "        # Define Encoder layer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=emb_dim, nhead=nhead, dim_feedforward=hidden_dim, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.fc = nn.Linear(emb_dim, 2)\n",
    "    def forward(self, x):\n",
    "        emb = self.pe(self.embed(x))\n",
    "        out = self.encoder(emb).mean(dim=1)\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bdc71579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Training and Evaluation\n",
    "\n",
    "# Train the model for one epoch\n",
    "def train_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    # Iterate over batches from the DataLoader\n",
    "    for x,y in loader:\n",
    "        x,y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    # Return average loss for the epoch\n",
    "    return total_loss/len(loader)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "def eval_model(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, correct = 0, 0\n",
    "    # Disable gradient tracking during evaluation\n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            total_loss += criterion(logits, y).item()\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds==y).sum().item()\n",
    "    # Return average loss and accuracy\n",
    "    return total_loss/len(loader), correct/len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588c81be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PangLee] DNN — train_time: 2.10s, loss: 0.666, acc: 0.568\n",
      "[PangLee] LSTM — train_time: 92.90s, loss: 0.807, acc: 0.542\n",
      "[PangLee] CNN — train_time: 14.88s, loss: 0.676, acc: 0.570\n",
      "[PangLee] Transformer — train_time: 73.59s, loss: 0.704, acc: 0.590\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Train and Evaluate Models for Movie Reviews\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"DNN\": DNN(len(itos), EMBED_DIM, HIDDEN_DIM, DROPOUT_PROB),\n",
    "    \"LSTM\": LSTMModel(len(itos), EMBED_DIM, HIDDEN_DIM, DROPOUT_PROB),\n",
    "    \"CNN\": CNNModel(len(itos), EMBED_DIM, HIDDEN_DIM, DROPOUT_PROB),\n",
    "    \"Transformer\": TransformerModel(len(itos), EMBED_DIM, HIDDEN_DIM, DROPOUT_PROB)\n",
    "}\n",
    "\n",
    "# Define the loss function for classification\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Iterate over each model to train and evaluate\n",
    "for name, model in models.items():\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "    start_time = time.time()\n",
    "    # Train the model for EPOCHS\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion)\n",
    "    train_time = time.time() - start_time\n",
    "    val_loss, val_acc = eval_model(model, test_loader, criterion)\n",
    "    print(f\"[PangLee] {name} — train_time: {train_time:.2f}s, loss: {val_loss:.3f}, acc: {val_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e19ff51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Yelp] DNN — train_time: 5.43s, loss=0.431, acc=0.810\n",
      "[Yelp] LSTM — train_time: 104.29s, loss=0.566, acc=0.767\n",
      "[Yelp] CNN — train_time: 66.98s, loss=0.381, acc=0.830\n",
      "[Yelp] Transformer — train_time: 364.74s, loss=0.461, acc=0.814\n"
     ]
    }
   ],
   "source": [
    "# Train and Evaluate Models for Yelp Reviews\n",
    "\n",
    "# Iterate over each model to train and evaluate\n",
    "for name, model in models.items():\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "    start_time = time.time()\n",
    "    # Train the model for EPOCHS\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        train_epoch(model, y_train_loader, optimizer, criterion)\n",
    "    train_time = time.time() - start_time\n",
    "    val_loss, val_acc = eval_model(model, y_test_loader, criterion)\n",
    "    print(f\"[Yelp] {name} — train_time: {train_time:.2f}s, loss={val_loss:.3f}, acc={val_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9f96db0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained GloVe embeddings and initialize the embedding matrix for the model\n",
    "\n",
    "import gensim.downloader as api\n",
    "import torch\n",
    "\n",
    "# Load 100-d GloVe vectors\n",
    "wv = api.load(\"glove-wiki-gigaword-100\")\n",
    "\n",
    "vocab_size = len(itos)\n",
    "emb_matrix = torch.randn(vocab_size, EMBED_DIM)\n",
    "emb_matrix[0] = torch.zeros(EMBED_DIM)\n",
    "\n",
    "# Replace random embeddings with GloVe vectors where available\n",
    "for idx, token in enumerate(itos):\n",
    "    if token in wv:\n",
    "        emb_matrix[idx] = torch.tensor(wv[token])\n",
    "\n",
    "model = DNN(vocab_size, EMBED_DIM, HIDDEN_DIM, DROPOUT_PROB).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3f4c4d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PangLee Experiment: rand-emb ===\n",
      "rand-emb: train_time=2.43s, loss=0.668, acc=0.600\n",
      "\n",
      "=== PangLee Experiment: glove-fz ===\n",
      "glove-fz: train_time=1.86s, loss=0.693, acc=0.500\n",
      "\n",
      "=== PangLee Experiment: glove-ft ===\n",
      "glove-ft: train_time=2.22s, loss=0.606, acc=0.693\n",
      "\n",
      "=== PangLee Experiment: tfidf-avg ===\n",
      "tfidf-avg: train_time=0.16s, loss=0.563, acc=0.708\n",
      "\n",
      "PangLee Feature‑Engineering Summary:\n",
      "rand-emb: time=2.43s, acc=0.600\n",
      "glove-fz: time=1.86s, acc=0.500\n",
      "glove-ft: time=2.22s, acc=0.693\n",
      "tfidf-avg: time=0.16s, acc=0.708\n"
     ]
    }
   ],
   "source": [
    "# PangLee Movie Review Dataset Feature‐Engineering Experiments\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Dataset class that stores TF-IDF features and labels as tensors\n",
    "class FeatureDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        if not torch.is_tensor(features):\n",
    "            features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.features = features\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.features.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "# DNN model that takes averaged document embeddings\n",
    "class DNN_Avg(nn.Module):\n",
    "    def __init__(self, emb_dim, hidden_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.fc1  = nn.Linear(emb_dim, hidden_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.fc2  = nn.Linear(hidden_dim, 2)\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.drop(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "# Create a TF-IDF vectorizer and transform the training and test texts\n",
    "vectorizer = TfidfVectorizer(vocabulary=itos)\n",
    "tfidf_train = vectorizer.fit_transform(train_texts)\n",
    "tfidf_test = vectorizer.transform(test_texts)\n",
    "emb_matrix_np = emb_matrix.cpu().numpy()  \n",
    "\n",
    "# Normalize and scale the embedding matrix\n",
    "mu, sigma = emb_matrix.mean(), emb_matrix.std()\n",
    "emb_matrix = (emb_matrix - mu) / (sigma + 1e-9)    \n",
    "emb_matrix *= 0.01               \n",
    "model.embed.weight.data.copy_(emb_matrix)\n",
    "\n",
    "# Compute document embeddings\n",
    "doc_emb_train = tfidf_train.dot(emb_matrix_np)\n",
    "doc_emb_test = tfidf_test.dot(emb_matrix_np)\n",
    "\n",
    "# Create datasets and dataloaders for TF-IDF features\n",
    "tfidf_train_ds = FeatureDataset(doc_emb_train, train_labels)\n",
    "tfidf_test_ds = FeatureDataset(doc_emb_test,  test_labels)\n",
    "tfidf_train_loader = DataLoader(tfidf_train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "tfidf_test_loader = DataLoader(tfidf_test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "results_feat = {}\n",
    "\n",
    "configs = {\n",
    "    \"rand-emb\": {\"type\":\"emb\",\"init\":\"random\",\"freeze\":False},\n",
    "    \"glove-fz\": {\"type\":\"emb\",\"init\":\"glove100d\",\"freeze\":True},\n",
    "    \"glove-ft\": {\"type\":\"emb\",\"init\":\"glove100d\",\"freeze\":False},\n",
    "    \"tfidf-avg\":{\"type\":\"tfidf-emb\"}\n",
    "}\n",
    "\n",
    "# Run experiment for each configuration\n",
    "for name, cfg in configs.items():\n",
    "    print(f\"\\n=== PangLee Experiment: {name} ===\")\n",
    "    if cfg[\"type\"] == \"emb\":\n",
    "        model = DNN(len(itos), EMBED_DIM, HIDDEN_DIM, DROPOUT_PROB).to(device)\n",
    "        if cfg[\"init\"] == \"glove100d\":\n",
    "            model.embed.weight.data.copy_(emb_matrix)\n",
    "        if cfg[\"freeze\"]:\n",
    "            model.embed.weight.requires_grad = False\n",
    "        else:\n",
    "            model.embed.weight.requires_grad = True\n",
    "        train_ld, test_ld = train_loader, test_loader\n",
    "    else:\n",
    "        model = DNN_Avg(EMBED_DIM, HIDDEN_DIM, DROPOUT_PROB).to(device)\n",
    "        train_ld, test_ld = tfidf_train_loader, tfidf_test_loader\n",
    "\n",
    "    optimizer = optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()), lr=LR\n",
    "    )\n",
    "    \n",
    "    start = time.time()\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        train_epoch(model, train_ld, optimizer, criterion)\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    loss, acc = eval_model(model, test_ld, criterion)\n",
    "    print(f\"{name}: train_time={elapsed:.2f}s, loss={loss:.3f}, acc={acc:.3f}\")\n",
    "    results_feat[name] = {\"time\": elapsed, \"loss\": loss, \"acc\": acc}\n",
    "\n",
    "print(\"\\nPangLee Feature‑Engineering Summary:\")\n",
    "for name, res in results_feat.items():\n",
    "    print(f\"{name}: time={res['time']:.2f}s, acc={res['acc']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923191fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Yelp Experiment: rand-emb ===\n",
      "rand-emb: train_time=6.08s, loss=0.452, acc=0.792\n",
      "\n",
      "=== Yelp Experiment: glove-fz ===\n",
      "glove-fz: train_time=3.19s, loss=0.664, acc=0.605\n",
      "\n",
      "=== Yelp Experiment: glove-ft ===\n",
      "glove-ft: train_time=5.56s, loss=0.392, acc=0.837\n",
      "\n",
      "=== Yelp Experiment: tfidf-avg ===\n",
      "tfidf-avg: train_time=0.85s, loss=0.546, acc=0.729\n",
      "\n",
      "Yelp Feature‑Engineering Summary:\n",
      "rand-emb: time=6.08s, acc=0.792\n",
      "glove-fz: time=3.19s, acc=0.605\n",
      "glove-ft: time=5.56s, acc=0.837\n",
      "tfidf-avg: time=0.85s, acc=0.729\n"
     ]
    }
   ],
   "source": [
    "# Yelp Dataset Feature‐Engineering Experiments\n",
    "\n",
    "vectorizer_y = TfidfVectorizer(vocabulary=itos)\n",
    "tfidf_y_train = vectorizer_y.fit_transform(y_train_texts)\n",
    "tfidf_y_test = vectorizer_y.transform(y_test_texts)\n",
    "\n",
    "emb_matrix_np = emb_matrix.cpu().numpy()\n",
    "\n",
    "doc_emb_y_train = tfidf_y_train.dot(emb_matrix_np)\n",
    "doc_emb_y_test = tfidf_y_test.dot(emb_matrix_np)\n",
    "\n",
    "y_tfidf_train_ds = FeatureDataset(doc_emb_y_train, y_train_labels)\n",
    "y_tfidf_test_ds = FeatureDataset(doc_emb_y_test,  y_test_labels)\n",
    "y_tfidf_train_loader = DataLoader(y_tfidf_train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "y_tfidf_test_loader = DataLoader(y_tfidf_test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "results_yelp = {}\n",
    "\n",
    "for name, cfg in configs.items():\n",
    "    print(f\"\\n=== Yelp Experiment: {name} ===\")\n",
    "    \n",
    "    if cfg[\"type\"] == \"emb\":\n",
    "        model = DNN(len(itos), EMBED_DIM, HIDDEN_DIM, DROPOUT_PROB).to(device)\n",
    "        if cfg[\"init\"] == \"glove100d\":\n",
    "            model.embed.weight.data.copy_(emb_matrix)\n",
    "        if cfg[\"freeze\"]:\n",
    "            model.embed.weight.requires_grad = False\n",
    "        else:\n",
    "            model.embed.weight.requires_grad = True\n",
    "        train_ld, test_ld = y_train_loader, y_test_loader\n",
    "    else:\n",
    "        model = DNN_Avg(EMBED_DIM, HIDDEN_DIM, DROPOUT_PROB).to(device)\n",
    "        train_ld, test_ld = y_tfidf_train_loader, y_tfidf_test_loader\n",
    "\n",
    "    optimizer = optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()), \n",
    "        lr=LR\n",
    "    )\n",
    "    \n",
    "    start = time.time()\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        train_epoch(model, train_ld, optimizer, criterion)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    loss, acc = eval_model(model, test_ld, criterion)\n",
    "    print(f\"{name}: train_time={elapsed:.2f}s, loss={loss:.3f}, acc={acc:.3f}\")\n",
    "    results_yelp[name] = {\"time\": elapsed, \"loss\": loss, \"acc\": acc}\n",
    "\n",
    "print(\"\\nYelp Feature‑Engineering Summary:\")\n",
    "for name, res in results_yelp.items():\n",
    "    print(f\"{name}: time={res['time']:.2f}s, acc={res['acc']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6b245f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer+GloVe‑ft on Movie Reviews: train_time=306.12s, loss=2.092, acc=0.675\n",
      "Transformer+GloVe‑ft on Yelp Reviews: train_time=724.93s, loss=0.644, acc=0.818\n"
     ]
    }
   ],
   "source": [
    "# Improved System: Transformer + GloVe‑ft\n",
    "\n",
    "# Initialize TransformerModel with GloVe‑fine-tuning\n",
    "model = TransformerModel(\n",
    "    vocab_size=len(itos),\n",
    "    emb_dim=EMBED_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    dropout=DROPOUT_PROB,\n",
    "    nhead=4,\n",
    "    num_layers=2\n",
    ").to(device)\n",
    "\n",
    "# Copy precomputed GloVe emb_matrix and enable fine-tuning\n",
    "model.embed.weight.data.copy_(emb_matrix)\n",
    "model.embed.weight.requires_grad = True\n",
    "\n",
    "# Optimizer and criterion\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and time on movie reviews and Evaluate\n",
    "EPOCHS = 20\n",
    "start_time = time.time()\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion)\n",
    "train_time = time.time() - start_time\n",
    "val_loss, val_acc = eval_model(model, test_loader, criterion)\n",
    "print(f\"Transformer+GloVe‑ft on Movie Reviews: \"\n",
    "      f\"train_time={train_time:.2f}s, loss={val_loss:.3f}, acc={val_acc:.3f}\")\n",
    "\n",
    "# Repeat on Yelp Reviews\n",
    "EPOCHS = 10\n",
    "start_time = time.time()\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    train_epoch(model, y_train_loader, optimizer, criterion)\n",
    "y_train_time = time.time() - start_time\n",
    "\n",
    "y_loss, y_acc = eval_model(model, y_test_loader, criterion)\n",
    "print(f\"Transformer+GloVe‑ft on Yelp Reviews: \"\n",
    "      f\"train_time={y_train_time:.2f}s, loss={y_loss:.3f}, acc={y_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d712e89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1\n",
      "Index: 1\n",
      "True label: NEG\n",
      "Predicted label: POS\n",
      "Text snippet:\n",
      " \" spawn \" features good guys , bad guys , lots of fighting , bloody violence , a leather-clad machine gun chick , gooey , self-healing bullet holes , scatological humor and a man-eating monster .  it not only appears to have been tailor made for a swarm of 12- and 13-year-old boys , it appears to have been made by them .  in a classic example of telling and not showing , \" spawn \" opens with a truckload of mumbo jumbo about forces of darkness , forces of light and how \" men are the ones who create evil on earth . \"  so much for a message .  the movie then lurches forward into the plight of al simmons ( michael jai white ) , a government assassin/operative who is murdered by diabolical boss jason wynn ( martin sheen , who plays all of his scenes like an oscar clip ) while on a top secret mission in a north korean biological weapons plant .  simmons goes to hell and back , after making a deal with satan himself -- if he agrees to command the devil's army to overtake the world , he'll be allowed to return to earth to see his wife wanda ( underused theresa randle ) and little girl cyan ( sydni beaudoin ) .  of course , seeing as how five years has passed , wanda has fallen for -- and married -- simmons' partner ( d . b .  sweeney ) .  his , uh , shoulder to cry on comes in the form of clown ( john leguizamo ) , a disgustingly disproportioned minion of satan's .  clown manipulates simmons , now in superhuman spawn form , into a stand off with wynn .  wynn , who thinks he is in league with the double-dealing clown , recognizes spawn as a threat and undergoes an operation where a bomb is placed on his heart -- when it stops beating , major cities around the world will detonate , causing the leak of a disease that \" makes the ebola virus look like a skin rash . \"  phew . got all that ?  it would be easy to dismiss \" spawn \" as just another one of those heavy-on-fx , short-on-substance action pics , but it doesn't even work on that level .  the computer-generated sequences are often and plenty , and the problem is that they look too computer-generated .  the several scenes set in hell present a devil that looks and acts like a video game graphic -- with the movie's healthy budget you'd think they could have afforded to make his mouth move when he talks .  other elements of the movie are so-so ; spawn's enormous red , flowing cape is a wonderful sight , but it's too obvious when he's being played by a costumed actor or an image .  in movies like \" contact , \" the special effects serve the story .  in \" spawn , \" they are the story .  and spawn himself isn't even an interesting character .  the film's reliance on razzle-dazzle would be acceptable if we were given somebody to root for , but both simmons and his funky alter-ego are completely underdeveloped .  what we need is a batman , a luke skywalker .  even watching the adventures of kleenex man would be more interesting than spawn .  poor leguizamo .  he starred in february's \" the pest , \" a movie that i still think is the year's worst so far , although this one does give it a run for its money .  as clown , he overacts to the extreme , never missing an opportunity for a poor-taste punchline .  leguizamo farts green mist , munches a pizza slice covered with maggots and even dons a mini-skirt and performs a cheerleader routine , all before turning into a giant grey demon .  the guy was brilliant in \" to wong foo , thanks for everything !  julie newmar . \"  what is he doing wasting his talent in this and \" the pest \" ?  i'm one of the few people that liked \" batman & robin , \" this summer's other big-budget comic book film .  yet after catching this movie and making the inevitable comparison , i can only hope people will change their minds and think of \" batman \" as the superior adaptation .  there's a compelling story somewhere in \" spawn \" -- including strong religious overtones and the debut of the first african-american superhero ever -- but it's just not found anywhere near here .  as it is , \" spawn \" is just awful -- it stinks worse than a dead trout .   ...\n",
      "\n",
      "Example 2\n",
      "Index: 7\n",
      "True label: NEG\n",
      "Predicted label: POS\n",
      "Text snippet:\n",
      "synopsis : al simmons , top-notch assasin with a guilty conscience , dies in a fiery explosion and goes to hell .  making a pact with malebolgia , a chief demon there , simmons returns to earth 5 years later reborn as spawn , a general in hell's army donning a necroplasmic costume replete with knives , chains , and a morphing cape .  sullen , wise cogliostro and flatulating , wisecracking violator vy for spawn's attention .  comments : when todd mcfarlane left marvel comics ( where he had made a name for himself as a first-rate comic book penciller on the \" spider-man \" titles ) to join the newly-formed , creator-owned image comics , a new comic book legend was born : spawn .  mcfarlane's \" spawn \" immediately became a commercial and critical success and a defining comic book series of the 1990s .  mcfarlane created a hero who was not only original but visually intricate , allowing mcfarlane to utilize his knack for artistic detail to the max .  the early \" spawn \" issues brilliantly capture mcfarlane's genius at illustration and show his early attempts at writing .  with the popularity of \" spawn \" and the success of the current warner bros . 's batman film franchise , a movie version of some sort seemed inevitable for spawn .  in the summer of 1997 , hence , new line cinema released spawn , a live-action film based on the groundbreaking series .  this topheavy exercise in violence and special effects unfortunately topples quickly and leaves fans of the comic book , like me , numbed by how much spawn misses the mark .  what happened ?  why is spawn so bad ?  todd mcfarlane himself executive produced this disappointing misfire and even appears in a cameo .  i don't think , however , that his presence necessarily hurt ( or helped ) the film .  i place the blame , in part , on the recent hollywood trend , fueled by public demand apparently , for special effects blow-out movies utilizing the latest computer technology .  these films focus upon the effects at the expense of everything else : character , plot , dialogue , etc . spawn , reflecting this trend , shows the audience one gratuitous scene after another populated with morphing characters and filled with unnecessary pyrotechnics .  hardly a minute goes by in this film without fires , explosions , knives and chains appearing out of nowhere , glowing eyes , or constantly transforming demons .  a lot of it is visually interesting and technically solid , don't get me wrong , but , because the script and cast aren't engaging , spawn ultimately comes across like overwrought wallpaper ( the surface may capture the eye , but nothing exists underneath ) .  spawn's translation of the comic book suffers the most at the storyline level .  mcfarlane's spawn was a tortured hero .  a mercenary by trade , al simmons was nonetheless a warm man in love with the beautiful wanda .  having died and journeyed to hell , he made a pact to return to earth to be with wanda .  simmons , however , discovers that his memories are fragmented , his body a creepy mess , and his wife married .  despite his sometimes violent nature , readers couldn't help but feel sympathetic toward his plight as the spawn of the underworld .  spawn attempts to show all of this but does not spend nearly the time it should to do so .  when the characters are developed , they seem absurd rather than touching .  the cartoonish dialogue and implausible subplot ( a general possesses the antidote to a supervirus called heat-16 which he wishes to unleash to enslave the world ) do not help matters .  spawn , in an apparent attempt to duplicate the success of batman , also unwisely spends too much time on a villain , the violator ( batman favored the joker over batman ) .  john leguizamo , like jack nicholson in batman , receives top billing in the cast as the violator ; michael jai white ( al simmons / spawn ) is second .  i ordinarily find leguizamo an intensely annoying presence in films which seems to make him a perfect candidate for the violator .  the film , however , spends so much time on the violator's offensive antics that they grate on the nerves .  apparently meant to be the comic relief in the film ( as nicholson was in batman ) , especially when contrasted with the sullen spawn , the violator's lines are oftentimes grotesque and unfunny , leaving the audience wishing he would leave .  leguizamo does a satisfactory job in the role , but he is seen far too often in the film .  michael jai white , a relative newcomer to theatrical releases , seems to be an appealing actor , and he handles his role adequately , but we see little of him without various masks on .  more time needed to be spent on white's character before he became spawn for the movie to pull at the heartstrings .  a special note should be made about martin sheen as the over-the-top , obnoxious , evil general wynn .  easily the hammiest performance in the movie , it's hard to imagine how sheen mucked up his role so much ; after all , he played a vietnam assasin brilliantly in the great apocalypse now .  sheen's excessive demeanor do not help the audience accept him as a mastermind villain and comes as a surprise considering his extensive career in film .  many other elements conspire with the disappointing script and abundant special effects to drag spawn down .  mtv-style , jerky , in-your-face editing is one of them .  flames , for example , roll across the screen sometimes to announce a shift in setting .  cogliostro , unlikely wannabe guide for spawn , serves as a poor narrator for the film .  he goofily tells the audience , at one point , that \" how much of [spawn's] humanity is left remains to be seen , \" as if the audience really cares as one violent sequence leads to another .  the music , finally , assaults the audience as much as the manic violence and offensive dialogue .  loud and obnoxious hard rock fused with drum loops dominate some scenes .  to be fair , however , marilyn manson's \" long hard road out of hell \" effectively compliments spawn's return to earth , while filter and the crystal method's \" ( can't you ) trip like i do \" proves a surprisingly fitting theme song .  for as good a comic book as it is , \" spawn \" did not spawn a good movie .  spawn , instead , suffers from too much pomp and circumstance , and too little plot and character development .  it receives two stars for its technically well-done special effects .  many other films , though , have equal , if not superior , special effects and are much better .  rated pg-13 , spawn seems more violent than many r-rated movies and probably wouldn't be appropriate for the very young .   ...\n",
      "\n",
      "Example 3\n",
      "Index: 8\n",
      "True label: NEG\n",
      "Predicted label: POS\n",
      "Text snippet:\n",
      "way of the gun is brimming with surprises , some good , most bad .  one of the good ones is ryan phillippe's surprisingly halfway decent performance .  after the actor gained much attention by posing and preening through teen swill like i know what you did last summer , he hinted at a bit growth in last year's cruel intentions with his amusingly contemptuous john malkovich meets james spader performance , though his acting in that film faltered around the third act mark , precisely when the screenplay made his character grow a heart ( presumably to appeal to his training bra wearing fans ) and start bellyaching about how he'd fallen for his \" target \" .  it was a dramatic shift that neither phillippe nor the film's director could negotiate .  but he seems to be trying and that shouldn't be overlooked ( or probably over praised ) seeing as how , at this point , he really isn't required to do much but look pretty .  here , phillippe has procured a five day growth of beard , his hair askew and his affect altered to sound something like james cagney in all his \" look here , see \" glory .  it's tough to believe a pretty boy like phillippe as a hard ass , but his performance actually helps with the illusion .  unlike ben affleck's puppy dog approach in reindeer games , phillippe is believable and not too bad at that .  as for the plot , well , that's one of the bad surprises ; phillippe and the great benicio del torro play two moronic ( and not even remotely likeable ) criminals ( introduced to us in the parking lot of a rave where they stupidly pick a fight with about twenty people ) who hatch a scheme to kidnap a surrogate mother ( juliette lewis ) after overhearing that she's carrying the baby of painter ( scott wilson ) , a very well connected wealthy man .  the kidnapping devolves into a laborious shoot out where much damage is done and many are killed .  but the anti-heroes escape with the woman while an aging hit man ( james caan ) and two scheming bodyguards ( taye diggs and nicky katt ) remain in pursuit .  directed by christopher mcquarrie , the screenwriter who won an oscar for his work on usual suspects , an overrated piece of crime noir in many circles ( this one included ) , way of the gun is a hodgepodge of crime thriller motifs that just oozes eye rolling familiarity .  unlike phillipe , mcquarrie doesn't seem to be growing at all ; he fills his flick with a sprawling labyrinth of plot all snatched from movies i know i've seen before and worse , it feels like it .  the film lacks even a fresh approach ( like what soderbergh did out of sight or the limey ) to its clich ? s , all of which are spewed before us in a picture that tries so desperately to be hip and gritty without bothering to notice how common it is .  it features characters living by a code they seemed to have picked up in sam peckinpah 101 rather than any thing resembling life .  even attempts at emotional weight feel strained like in preposterous scene where enemies james caan and benicio del torro stop in their tracks to have a cup of coffee and pontificate on life , philosophy , etc . i didn't like the bit much when it first appeared in heat , here , it's even more self consciously \" dramatic \" .  usual suspects , which also featured a bunch of low lives that seemed to live by a code they learned from the movies , worked to some degree because of that astonishing twist ending .  nowadays an astonishing twist per ending is de rigeur , never mind if it deems all that transpired before it completely inconsequential .  thus , nearly every character in way comes with at least one dirty little secret ( most amateurishly projected before they appear ) and it too has an ironic little twist at the end ( nothing earth shattering , like in the usual suspects ) but i admit , i didn't see it coming , and i smiled .  however most of the movie feels exactly like the interrogation scenes between palminteri and spacey in usual suspects : a lot of faux huffing and puffing with no rhythm or reason .  or substance .  it's just as overplotted as suspects , and often dull as any overplotted movie without interesting characters , a distinctive style , or a good script would be .  even the good stuff , most of which involves james caan ( doing a fine job ) , feels odd and out of place in a movie that thinks lines like \" karma is only justice with out the satisfaction \" are clever .  at one point caan laments \" need is the ultimate monkey \" a line so inexplicable he might as well have said \" love is like hippo ass \" .  i've seen porno with better dialogue .  and some with better plot lines too .  but rarely has porn offered up such a talented cast ( i mean , can you even compare james caan to ron jeremy ) .  caan slips into this role with seeming ease , he could do this kind of soft-spoken tough guy in a coma , but he manages to give his character depth and weariness .  benicio del torro is always welcome , though here he plays it fairly straight ( rather than another oddball character creation like the ones the actor gave us in usual suspects and excess baggage ) , adopting a brad pitt-esque quizzical pout to go along with his heavy swaggering .  speaking of pitt , his ex , juliette lewis , is a weak link , either shouting her lines with ear shattering shrillness , or waddling about like a silly goose .  nicky katt was brilliant in a brief role in the limey , here his role is just as brief only he seems wasted ; he's only kept around for his cold presence .  taye diggs has a similar function , cool as ever , but never a character , though the guy has one helluva death scene .  for the most part way is incessantly talky with no reason to give a hippo's ass about any thing that transpires since its characters are never more than simplistic pawns .  it's vaguely tarantino-ish , but in a bad truth and consequences nm way , with phillippe , in one scene , leaping into a stupid rant about \" faggots \" migrating to los angeles .  you know the routine .  the final shoot out is pure bargain basement john woo ( who himself seems to be doing bargain basement john woo ) with interchangeable bad guys lining up to be shot at .  but those gun shots sure crack like thunder .  just the other day i was watching an old dirty harry movie on cable and was stunned to hear the weak elephant grunt-like sound that emanated from harry's fetishized smith and wesson .  oh how far we've come .   ...\n",
      "\n",
      "Example 4\n",
      "Index: 9\n",
      "True label: POS\n",
      "Predicted label: NEG\n",
      "Text snippet:\n",
      "seen february 15 , 1998 on home video ( borrowed from chris wessell ) .  when it comes to modern gangster movies , it's really difficult to describe and review them without making comparisons to other films of the genre and/or just using the word \" routine . \"  i've always subscribed to the philosophy that any idea ( no matter how many times it's been used before ) can provide for a good story and \" donnie brasco \" clinches this idea .  it's not unlike most of the great films of the genre , yet it never apes another's style as it has a good layer of authenticity , even if its core is a tad stale .  the film starts off in typical fashion by defining its atmosphere of new york city in the late 1970s and the mobsters who inhibit it .  we meet lefty ( pacino ) , an aging wiseguy who can still walk the walk and talk the talk .  he and his associates go through the generic motions you expect to see in crime films like this .  somehow he comes across donnie brasco ( depp ) , a younger guy with a lot of spunk who isn't afraid of lefty and his rep , and even manages to befriend him after lefty was ready to kill him .  it's clear donnie is new to the life and lefty recognizes this immediately , telling him all the tricks of the trade .  i have never seen this technique of actually revealing the mafioso idiosyncrasies done before and for this the film deserves credit .  however , we soon realize donnie is actually joe pistone , an fbi agent working undercover - a character who symbolizes the viewer as he will soon be purged into the lifestyle and treated as a newcomer .  the first act works as a guided to ot of rhetoric about wiseguy honor , a brief history of the mob , definitions of their slang , and where their money comes from and who it goes to .  the screenplay is rather sketchy on the details surrounding these elements , however , the fact they are mentioned at all is quite original .  most gangster movies seem to be made with the notion the viewer already knows how the mob works ( probably from watching other gangster movies ) , and although this attitude comes across , the film tries to fill in all the holes where and when it can and the effort is appreciable .  thankfully the film doesn't become too caught up in the tedious details of organized crime , and instead opts for character development .  much of the story is told simply through the interaction between lefty and donnie .  pacino is outstanding here as the pathetic hood who speaks of his job in the same manner any blue collar worker would .  he's old and exhausted but seems to enjoy what he does , just as anyone loyal to the same employer for over 30 years might be .  we learn of his accomplishments , which are quite impressive within their context , and when he complains about not being made top boss , it's easy to sympathize with him .  newell constantly plays up this aspect , making it a major theme which works well in the long run .  as donnie is constantly impressing and even one-upping lefty , it's hard to tell which emotion is more powerful : the fact donnie is getting closer to nailing the mob ; or the fact lefty has once again been over shadowed .  most of the film tells the story of donnie's life in mafia , which creates for many sub-plots and individual conflicts , but doesn't always seem to come together as a whole .  the storytelling is genuinely interesting throughout , even when the motions the characters go through seem familiar .  the resonance to donnie's actual assignment varies , he often reports back to the feds with detail of his progress , but it doesn't always seem to have much meaning .  back home , his wife maggie ( heche ) is ready to divorce him because he's never a and the family is suffering .  since this is based on a true story i wouldn't doubt this would happen , although the way it is handled often borders on the melodramatic .  my only major complaint is the film seems to have no final act , or at least any real sense of closure .  a climax of sorts does occur , but there's little feeling of a payoff .  we get a happy ending , which is good , but perhaps a sad ending would have been more powerful .  aside from a few minor , general flaws , \" donnie brasco \" manages to be a solid piece of storytelling and character development .  it may be routine , but it's good , and that's respectful .   ...\n",
      "\n",
      "Example 5\n",
      "Index: 11\n",
      "True label: POS\n",
      "Predicted label: NEG\n",
      "Text snippet:\n",
      " \" take a number , fill out a form , and wait your turn . \"  starring kati outinen , kari v ? ? n ? nen , sakari kuosmanen , elina salo ; written & directed by aki kaurism ? ki ; cinematography by timo salminen  it might be possible to call drifting clouds a satire or a black comedy , but that would imply a sense of anger , of vitriol , of energy ; drifting clouds is what you get when the rage and vitality are gone .  it is the sad , slow story of lauri and ilona , a married couple caught between the wheels of capitalism as it grinds inexorably onward .  he loses his job as a tram driver , because everyone drives cars nowadays .  within a couple of months , she loses her position as a head waiter , when her restaurant is bought out by a chain and the entire staff replaced .  a conversation early in the film reveals a lot about their situation .  lauri has surprised ilona by buying her a tv , which she greets with little enthusiasm .  she notes that they haven't finished paying for the bookshelves or the couch yet .  he says that in four years the payments will be done and then they can buy some books for the shelves .  it would pass as deadpan humour if it wasn't spoken with such resigned weariness .  this sets the tone for the rest of the movie .  there is humour and idiosyncracy at the periphery , but at the centre there is frustration and futility , and sorrow for the ways in which the logic of profit reduces people , until their worth is equated solely with their earning capacity .  there is an element of political commentary in this critique of the mechanisms of capitalism and the stultifying social environment it creates , but in drifting clouds the political is subsumed by the personal : the movie is about two people and what happens to them , and nothing else matters .  lauri and ilona live lives starved for friendship , respect , culture , passion .  they go to the movies , and walk out past old posters for l'atalante and l'argent , but the movie they have just seen is a pointless , violent , unfunny comedy .  their house and workplaces are uniformly unpleasant , painted and upholstered in lifeless colours ( ugly greens , dull reds , insipid blues ) , full of inelegantly functional objects and appliances .  the art design is impeccable in its tawdriness , and director aki kaurism ? ki often matches the colour of characters' clothes with the background colour and/or lighting , so that it seems as if they are almost physically fading into the environment .  there is no suggestion of sexuality in the relationship ; ilona and lauri sleep in separate beds and their gestures of affection lack the heat of desire ; their lives are now bound by something more complex and desperate than love .  their downward , downsized , downtrodden lives have a momentum that is almost comic , as one setback succeeds another , bottoming out when lauri stakes all their remaining money on the spin of a roulette wheel .  it must have been tempting to play the story for more laughs , more farce , more deadpan wit , but the film's great strength is the sober , empathic manner in which it observes lauri and ilona's misfortunes .  irony would be an injustice .  situations and settings are broad and exaggerated--this is not realism--but they are not distorted .  the emotions are authentic .  if it is difficult to laugh at anything that happens , despite the droll performances , the laconic humour , that's because kaurism ? ki brings such compassion and understanding to the manifold indignities that are suffered .  he shows us how humiliating it must be for a woman of thirty-eight , who has worked long and hard to win a respectable position , to be forced to accept a job as a dishwasher in a two-bit restaurant ; how humiliating it must be for a man nearing 50 to confront his wife's former employer , demanding the rest of her wages , only to have the crap beaten out of him , unable to land a single punch , when the employer and his cronies refuse to give him the money .  all this is observed keenly , with great economy : every cut , every line of dialogue , is judicious .  if most films are novelistic in their telling , this one brings the focus and concentration of a short story to bear .  but what is gained in nuance and acumen is rather undermined by the sense that the material barely accommodates the 96-minute running length .  i have seen many movies far less profound , less humane , less necessary than drifting clouds , but they filled me with an urge to watch them again , and this one did not .  it does not need to be seen twice : its every detail and implication can be absorbed in one viewing .  it is not a movie that will be seen by a large audience , because it cannot be pitched to one .  there is no selling point .  it is all understatement , restraint , melancholy .  the characters are unremarkable , their best years behind them , their dreams dissipated ; it takes all the effort they can muster just to pay the bills .  this does not make them less fascinating , merely less marketable , which is a shame , because this is a movie which should be seen , precisely because it pays attention to people and emotions that most movies prefer to ignore .  it engages us and touches us and resolves--surprisingly , and movingly--into something resembling a happy ending .  the only thing greater than the ill-luck that governs the characters' lives is their refusal to give in to despair .  their persistence is rewarded with what might be called a \" feel-good \" ending elsewhere , but not here , because this ending differs in kind from most such endings : this one has truly been earned .   ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Error Analysis on Movie Review: Find 5 Misclassified Examples\n",
    "\n",
    "model.eval()\n",
    "misclassified = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, text in enumerate(test_texts):\n",
    "        # Encode and move to device\n",
    "        x = torch.tensor(encode(text, stoi), dtype=torch.long).unsqueeze(0).to(device)\n",
    "        logits = model(x)\n",
    "        pred = logits.argmax(dim=1).item()\n",
    "        true = test_labels[i]\n",
    "        if pred != true:\n",
    "            misclassified.append((i, text, true, pred))\n",
    "        if len(misclassified) >= 5:\n",
    "            break\n",
    "\n",
    "# Print the first 5 misclassified examples\n",
    "for idx, (i, text, true, pred) in enumerate(misclassified, 1):\n",
    "    print(f\"Example {idx}\")\n",
    "    print(f\"Index: {i}\")\n",
    "    print(f\"True label: {'POS' if true==1 else 'NEG'}\")\n",
    "    print(f\"Predicted label: {'POS' if pred==1 else 'NEG'}\")\n",
    "    print(\"Text snippet:\")\n",
    "    print(text.replace(\"\\n\",\" \"), \"...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bdb3ba05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1\n",
      "Index: 0\n",
      "True label: NEGATIVE\n",
      "Predicted label: POSITIVE\n",
      "Text snippet:\n",
      "Severely overrated place. Their sugar-coated corn pancake topped with green onions was the most confusing appetizer I'd ever had. Their side dishes lack variety and freshness, chicken cutlet from their bentos was awfully hard, and their dak-bokkum-tang was a disaster. I tried here because my favorite teacher was a friend of the manager, but sorry I'm not going there again. I get why people like it though; a good place to start if you have zero tolerance for the authentic savory Korean flavor and all you need is an instagram photo of 'exotic' food. ...\n",
      "\n",
      "Example 2\n",
      "Index: 6\n",
      "True label: NEGATIVE\n",
      "Predicted label: POSITIVE\n",
      "Text snippet:\n",
      "I thought this was going to be the Za's from a couple years back but it is not. They took a proven concept and have butchered it with subpar ingredients and the use of a microwave. They used to cook the pasta with the sauce along with the ingredients which led to a great product that I was willing to pay for. This new Za's is putting ingredients on pasta, microwaving it, then pouring sauce and spices on after. I don't know who thought of this cooking process but it wasn't someone who knows how to cook. It was a bad experience, and now I feel a bit sick from it. If you value your stomach and tastebuds do not go to this establishment. ...\n",
      "\n",
      "Example 3\n",
      "Index: 9\n",
      "True label: NEGATIVE\n",
      "Predicted label: POSITIVE\n",
      "Text snippet:\n",
      "After going to Sushi Avenue, man does this place drop in my book. I already have serious qualms with that guy at the counter. And now I realize that Sushi Ichiban has the aesthetic appeal of a hospital waiting room. Unless you want a foam box full of average teriyaki, save your prime dollars for a sushi place that is worth going to... I'll give you a hint, it's on Green Street. ...\n",
      "\n",
      "Example 4\n",
      "Index: 13\n",
      "True label: NEGATIVE\n",
      "Predicted label: POSITIVE\n",
      "Text snippet:\n",
      "Restaurant review I recently visited this restaurant with a friend that came from out of town. The server was friendly and gave us plenty of time to order. While you wait you get complimentary popcorn with seasoning. Although it is good, I can't say I would fall head over heels for this. I ended up getting the sea scallops. Although I think they were fresh, the mixture of ingredients including blood orange and fennel puree was okay, but on the salty side. The highlight of the meal was the bread pudding that came out piping hot and was the perfect blend of chocolate and vanilla cream. If you like farm to table this is your restaurant, but I think the food is okay and still does not reach to the level of some of the fancier restaurants in chicago  Pros: farm to table food, friendly server, large beer selection, free parking validation  Cons: The food is okay, not superb  Hidden deals: Monday Night- Date Night $50 for two people four course Prix Fixe menu  Tuesday night- fried chicken and waffles $17 Wednesday Night- \"Five for $5\" Cocktail Specials Thursday Happy Hour- $1 off 16oz Draft Beers Friday Lunch- Fish Sandwich Special 10.99 Friday & Saturday Poutine after 9:00pm Sunday Night - 1/2 off Wine by the Glass  Health code rating: B. Clean restaurant, food appears to be prepared well. <a href=\"/redir?url=http%3A%2F%2Fchampaign.il.gegov.com%2Fchampaign%2Festab.cfm%3FfacilityID%3D1820&s=c247c24578938fbf3ef4413bb8d6b6862c0a6c27ef1486e38e249dbf3b577851\" target=\"_blank\" rel=\"nofollow\">champaign.il.gegov.com/c…</a> ...\n",
      "\n",
      "Example 5\n",
      "Index: 17\n",
      "True label: NEGATIVE\n",
      "Predicted label: POSITIVE\n",
      "Text snippet:\n",
      "This place has so much potential, so it's kind of sad that they don't have better service....   My fiance and I waited at the bar for a drink for about 15 minutes before anyone asked us what we wanted. That may not sound too long, but the place wasn't even busy. And what's funny is the bartender was like purposely ignoring us. There's no way she didn't see us politely flagging her down. She had a soft drink that she was drinking, and she stopped right in front of us, took a nice looooong drink... kind of relaxing, then she looked past us out the window, and proceeded to ignore us a bit longer. And its not like we were just standing there, you can try and flag them down, but they just ignore you like alien possessed humans staring into space or something.   The bartender finally asked what we wanted, but by then we were being seated so we said nevermind. Then we proceeded to have terrible service at our table, to the point that it was just kind of amusing. Oh and my cobb salad had these huge mushroom tentacle things that made me gag and we just sorta wiggled them at our friends like \"look at this huge sick thing that's in my salad!\" And the sliders are way over priced. The cheese curds were good, but a little heavy.   I say the place has potential because it's a really nice location and the inside has a nice ambiance. There is also an extensive beer list, but none of that really matters if your service is that bad... ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Error Analysis on Yelp Reviews: Find 5 Misclassified Examples\n",
    "\n",
    "model.eval()\n",
    "misclassified_yelp = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, text in enumerate(y_test_texts):\n",
    "        # Encode text and move to device\n",
    "        x = torch.tensor(encode(text, stoi), dtype=torch.long).unsqueeze(0).to(device)\n",
    "        logits = model(x)\n",
    "        pred = logits.argmax(dim=1).item()\n",
    "        true = y_test_labels[i]\n",
    "        if pred != true:\n",
    "            misclassified_yelp.append((i, text, true, pred))\n",
    "        if len(misclassified_yelp) >= 5:\n",
    "            break\n",
    "\n",
    "# Print the first 5 misclassified examples\n",
    "for idx, (i, text, true, pred) in enumerate(misclassified_yelp, 1):\n",
    "    print(f\"Example {idx}\")\n",
    "    print(f\"Index: {i}\")\n",
    "    print(f\"True label: {'POSITIVE' if true==1 else 'NEGATIVE'}\")\n",
    "    print(f\"Predicted label: {'POSITIVE' if pred==1 else 'NEGATIVE'}\")\n",
    "    print(\"Text snippet:\")\n",
    "    print(text.replace(\"\\n\",\" \"), \"...\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
