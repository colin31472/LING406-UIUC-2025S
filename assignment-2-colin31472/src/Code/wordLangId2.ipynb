{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 99.00%\n"
     ]
    }
   ],
   "source": [
    "### Youngjun Yu wordLangId2.ipynb ###\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Preprocessing and Update Function\n",
    "\n",
    "def preprocess(sentence):\n",
    "    \"\"\"\n",
    "    Preprocess a sentence by converting to lower-case and extracting only alphanumeric tokens. A word is defined as a contiguous sequence of letters and digits.\n",
    "    \"\"\"\n",
    "    sentence = sentence.lower()\n",
    "    tokens = re.findall(r'[a-z0-9]+', sentence)\n",
    "    return tokens\n",
    "\n",
    "def update_counts(sentence, unigram, bigram):\n",
    "    \"\"\"\n",
    "    Update unigram and bigram counts for a given sentence. '<s>' and '</s>' are added to mark start and end.\n",
    "    \"\"\"\n",
    "    tokens = preprocess(sentence)\n",
    "    if not tokens:\n",
    "        return\n",
    "    tokens = ['<s>'] + tokens + ['</s>']\n",
    "    for i in range(len(tokens)):\n",
    "        unigram[tokens[i]] += 1\n",
    "        if i != 0:\n",
    "            bigram[(tokens[i-1], tokens[i])] += 1\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Training the Model\n",
    "\n",
    "def train_model(filepath):\n",
    "    \"\"\"\n",
    "    Train a language model from the input.\n",
    "    \"\"\"\n",
    "    unigram = defaultdict(int)\n",
    "    bigram = defaultdict(int)\n",
    "    with open(filepath, encoding='utf8') as f:\n",
    "        for sentence in f:\n",
    "            sentence = sentence.strip()\n",
    "            if sentence:\n",
    "                update_counts(sentence, unigram, bigram)\n",
    "    return unigram, bigram\n",
    "\n",
    "def get_vocab(unigram):\n",
    "    \"\"\"\n",
    "    The vocabulary is the set of word tokens observed in the input data.\n",
    "    \"\"\"\n",
    "    return set(unigram.keys())\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Computing Bigram Log-Probability\n",
    "\n",
    "def compute_log_prob(sentence, unigram, bigram, vocab):\n",
    "    \"\"\"\n",
    "    Compute the log probability of a sentence using Good-Turing smoothing.\n",
    "    \"\"\"\n",
    "    tokens = preprocess(sentence)\n",
    "    if not tokens:\n",
    "        return float('-inf')\n",
    "    tokens = ['<s>'] + tokens + ['</s>']\n",
    "    \n",
    "    log_prob = 0.0\n",
    "    \n",
    "    k = 5  # threshold\n",
    "    \n",
    "    for i in range(1, len(tokens)):\n",
    "        r = bigram.get((tokens[i-1], tokens[i]), 0)\n",
    "        ft_context = bigram.get('ft', {}).get(tokens[i-1], None)\n",
    "        denom = bigram.get('denom', {}).get(tokens[i-1], None)\n",
    "        \n",
    "        if ft_context is not None and denom is not None:\n",
    "            if r < k and ft_context.get(r, 0) > 0 and ft_context.get(r+1, 0) > 0:\n",
    "                prob = (r + 1) * (ft_context[r+1] / ft_context[r]) / denom\n",
    "            else:\n",
    "                prob = r / denom\n",
    "        else:\n",
    "            prob = (r + 1) / (unigram.get(tokens[i-1], 0) + len(vocab))\n",
    "            \n",
    "        if prob == 0:\n",
    "            prob = 1e-10  # to avoid log(0)\n",
    "        log_prob += math.log(prob)\n",
    "        \n",
    "    return log_prob\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Compute Context Frequency-of-Frequency and Denominator for Good-Turing Smoothing\n",
    "\n",
    "def compute_ft_denom(model):\n",
    "    \"\"\"\n",
    "    For each previous token, compute a frequency-of-frequency table (context_ft) and the total adjusted count (denom).\n",
    "    \"\"\"\n",
    "    k = 5  # threshold\n",
    "    \n",
    "    context_ft = {}\n",
    "    denom = {}\n",
    "    for prev in model['unigram'].keys():\n",
    "        ft = defaultdict(int)\n",
    "        for next in model['vocab']:\n",
    "            r = model['bigram'].get((prev, next), 0)\n",
    "            ft[r] += 1\n",
    "        context_ft[prev] = dict(ft)\n",
    "        \n",
    "        total_adjusted = 0.0\n",
    "        for next in model['vocab']:\n",
    "            r = model['bigram'].get((prev, next), 0)\n",
    "            if r < k and ft.get(r, 0) > 0 and ft.get(r+1, 0) > 0:\n",
    "                total_adjusted += (r+1) * (ft[r+1] / ft[r])\n",
    "            else:\n",
    "                total_adjusted += r\n",
    "        denom[prev] = total_adjusted\n",
    "    return context_ft, denom\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Main: Train Models\n",
    "\n",
    "train_path = os.path.join(\"..\", \"Data\", \"Input\")\n",
    "models = {}\n",
    "\n",
    "for lang in ['English', 'French', 'Italian']:\n",
    "    filepath = os.path.join(train_path, 'LangId.train.' + lang)\n",
    "    unigram, bigram = train_model(filepath)\n",
    "    vocab = get_vocab(unigram)\n",
    "    models[lang] = {'unigram': unigram, 'bigram': bigram, 'vocab': vocab}\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Main: Test Models on Validation Data\n",
    "\n",
    "test_file = os.path.join(\"..\", \"Data\", \"Validation\", \"LangId.test\")\n",
    "results = []\n",
    "\n",
    "# Precompute Good-Turing data and Store in the Bigram Dictionary.\n",
    "for lang in ['English', 'French', 'Italian']:\n",
    "    ft, denom = compute_ft_denom(models[lang])\n",
    "    models[lang]['bigram']['ft'] = ft\n",
    "    models[lang]['bigram']['denom'] = denom\n",
    "\n",
    "with open(test_file, encoding='utf8') as f:\n",
    "    sentences = f.readlines()\n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        sentence = sentence.strip()\n",
    "        lang_probs = {}\n",
    "        for lang in ['English', 'French', 'Italian']:\n",
    "            model = models[lang]\n",
    "            log_prob = compute_log_prob(sentence, model['unigram'], model['bigram'], model['vocab'])\n",
    "            lang_probs[lang] = log_prob\n",
    "        predicted_lang = max(lang_probs, key=lang_probs.get)\n",
    "        results.append(f\"{idx+1} {predicted_lang}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Main: Save the Results\n",
    "\n",
    "output_file = os.path.join(\"..\", \"Data\", \"Output\", \"wordLangId2.out\")\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "with open(output_file, 'w', encoding='utf8') as f:\n",
    "    f.write(\"\\n\".join(results))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Main: Evaluate the Model by Computing Accuracy\n",
    "\n",
    "solution_file = os.path.join(\"..\", \"Data\", \"Validation\", \"labels.sol\")\n",
    "with open(output_file, encoding='utf8') as f_out:\n",
    "    output_sentences = []\n",
    "    for sentence in f_out:\n",
    "        sentence = sentence.strip()\n",
    "        if sentence:\n",
    "            output_sentences.append(sentence)\n",
    "\n",
    "with open(solution_file, encoding='utf8') as f_sol:\n",
    "    solution_sentences = []\n",
    "    for sentence in f_sol:\n",
    "        sentence = sentence.strip()\n",
    "        if sentence:\n",
    "            solution_sentences.append(sentence)\n",
    "\n",
    "correct = 0\n",
    "total = len(solution_sentences)\n",
    "for output, sol in zip(output_sentences, solution_sentences):\n",
    "    if output == sol:\n",
    "        correct += 1\n",
    "\n",
    "accuracy = correct / total * 100\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
